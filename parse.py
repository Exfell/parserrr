import asyncio
import csv
import random
import sys
import gzip
import io
import aiohttp
import pandas as pd
import re
import json
import time
from fake_useragent import UserAgent
import math
import logging

# –†–∞–Ω—å—à–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –±—ã–ª , - —Å–µ–π—á–∞—Å ;... –ù–∞–¥–æ —Å–ø—Ä–æ—Å–∏—Ç—å, –Ω–∞ –∫–∞–∫–æ–π –Ω–∞–¥–æ.
#–ü–æ–º–µ–Ω—è–ª divide (—á—Ç–æ–±—ã –ø–∏—Å–∞–ª) –∏ parse (—á—Ç–æ–±—ã –±–µ–∑ –∞—Ä—Ç–∏–∫—É–ª–µ–π)
#–ö–∞—Ä–æ—á–µ, –µ—Å–ª–∏ —Ö–æ—á–µ—à—å –≤–µ—Ä—Å–∏—é —Å query_count, —Ç–æ —Å–º–æ—Ç—Ä–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –Ω–∞–∑–∞–¥.
# cd /www/parserrr
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') #logging.DEBUG —á—Ç–æ–±—ã –æ—Ç–æ–±—Ä–∞–∂–∞–ª–æ—Å—å, .INFO - –Ω–µ—Ç
logger = logging.getLogger(__name__)


def sanitize_filename(filename: str) -> str:
    return re.sub(r'[<>:"/\\|?*]', '_', filename).strip()


async def fetch(session, keyword, semaphore, user_agent, query_count, retries=5):
    for attempt in range(retries):
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º –∑–∞–ø—Ä–æ—Å–æ–º
            await asyncio.sleep(random.uniform(0.5, 1.5))

            headers = {
                'User-Agent': user_agent,
                'Accept': 'application/json, text/plain, */*',
                'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
                'Referer': 'https://www.wildberries.ru/',
                'Connection': 'keep-alive',
                'Origin': 'https://www.wildberries.ru',
                'Accept-Encoding': 'gzip, deflate',
            }

            url = f'https://www.wildberries.ru/__internal/u-search/exactmatch/ru/common/v18/search?ab_testid=new_optim&ab_testing=false&appType=1&curr=rub&dest=12358470&hide_dtype=11&inheritFilters=false&lang=ru&page=2&query={keyword.replace(" ", "%20")}&resultset=catalog&page=1&spp=30&suppressSpellcheck=false'
            #print(url)
            async with semaphore:
                async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=40)) as response:
                    if response.status != 200:
                        #print(f"‚ùå –°—Ç–∞—Ç—É—Å {response.status} –¥–ª—è '{keyword}'")
                        raise Exception(f"Status: {response.status}")
                    raw_data  = await response.read()
                    content_encoding = response.headers.get('Content-Encoding', '').lower()
                    if 'gzip' in content_encoding:
                        # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º gzip
                        try:
                            with gzip.GzipFile(fileobj=io.BytesIO(raw_data)) as f:
                                text = f.read().decode('utf-8')
                        except Exception as e:
                            print(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ gzip: {e}")
                            # –ü—Ä–æ–±—É–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                            text = raw_data.decode('utf-8', errors='ignore')

                    elif 'br' in content_encoding:
                        # –ï—Å–ª–∏ brotli, –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –¥–µ–∫–æ–¥–µ—Ä—ã
                        try:
                            # –ü—Ä–æ–±—É–µ–º –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
                            text = raw_data.decode('utf-8')
                        except:
                            text = raw_data.decode('utf-8', errors='ignore')
                    else:
                        # –î–ª—è gzip –∏–ª–∏ plain text
                        text = raw_data.decode('utf-8')

                    result = json.loads(text)
                    total = result.get("total", 0)


                    #print(f'‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –¥–ª—è "{keyword}": total = {total}')

                    return {"keyword": keyword, "query_count": query_count, "total": total}

        except Exception as e:
            error_message = str(e)
            #print(f'‚ùå –û—à–∏–±–∫–∞ –¥–ª—è "{keyword}": {error_message}')

            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
            wait_time = (attempt + 1) * 3 + random.uniform(2, 5)
            await asyncio.sleep(wait_time)

    print(f"üö´ –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è '{keyword}'")
    return {"keyword": keyword, "query_count": query_count, "total": 0}




async def fetch_total(session: aiohttp.ClientSession, keywords: list, query_counts: list, semaphore: asyncio.Semaphore):
    ua_pool = [UserAgent().random for _ in range(10)]
    tasks = [asyncio.create_task(fetch(session, kw, semaphore, random.choice(ua_pool), query_count))
             for kw, query_count in zip(keywords, query_counts)]
    return await asyncio.gather(*tasks, return_exceptions=True)


async def scrape_all(keywords: list, concurrency: int = 120, query_counts: list = None):  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–æ 15
    semaphore = asyncio.Semaphore(concurrency)

    # –ï—â–µ –±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    conn = aiohttp.TCPConnector(
        limit=240, # –±—ã–ª–æ 20  –∏ 10 –Ω–∞ per_host
        limit_per_host=240,
        ssl=False,
        enable_cleanup_closed=True,
        force_close=True
    )
    timeout = aiohttp.ClientTimeout(total=60, connect=15, sock_connect=15, sock_read=30)

    async with aiohttp.ClientSession(connector=conn, timeout=timeout,auto_decompress=False) as session:
        return await fetch_total(session, keywords, query_counts, semaphore)

def save_results(results: list, filename: str, fileformats: list):
    df = pd.DataFrame(results)
    filename = sanitize_filename(filename)
    saved_files = []

    if "xlsx" in fileformats:
        df.to_excel(f'{filename}.xlsx', index=False)
        logger.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}.xlsx")
        saved_files.append(f'{filename}.xlsx')
    if "json" in fileformats:
        with open(f"{filename}.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
        logger.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}.json")
        saved_files.append(f'{filename}.json')
    if "csv" in fileformats:
        # –û—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ —Ç—Ä–∏ –∫–æ–ª–æ–Ω–∫–∏: –∑–∞–ø—Ä–æ—Å, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤, total
        df_filtered = df[["keyword", "query_count", "total"]]
        df_filtered.to_csv(f"{filename}", index=False, sep=',', encoding='utf-8-sig', errors='replace', header=False) # –∑–¥–µ—Å—å —É–∂–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–æ–≤—ã–π —Ñ–∞–π–ª. sep - –Ω–æ–≤—ã–π —Ñ–∞–π–ª, delimiter - —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª
        logger.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename} (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å ',')")
        saved_files.append(f'{filename}')

    return saved_files


def parse(filename_input: str, filename_out: str, fileformats: list = ('xlsx',), chunk_size: int = 1000,
          concurrency: int = 120):
    logger.info('–ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    with open(filename_input, "r", encoding="utf-8-sig") as f:
        reader = list(csv.reader(f,delimiter=';'))
        keywords_to_fetch = []    # –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥–µ–º –ø–∞—Ä—Å–∏—Ç—å (–Ω–µ –∞—Ä—Ç–∏–∫—É–ª—ã)
        query_counts_to_fetch = []
        skipped_results = []      # –¥–ª—è —Å—Ç—Ä–æ–∫-–∞—Ä—Ç–∏–∫—É–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–º total = 0

        for row in reader:
            if row and row[0].strip():
                keyword = row[0].strip()
                query_count = row[1].strip() if len(row) > 1 else ""
                # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å–æ—Å—Ç–æ–∏—Ç —Ç–æ–ª—å–∫–æ –∏–∑ —Ü–∏—Ñ—Ä (—Å —É–¥–∞–ª—ë–Ω–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏) ‚Äì —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ –∞—Ä—Ç–∏–∫—É–ª
                if keyword.replace(" ", "").isdigit() and len(keyword.replace(" ", "")) > 6:
                    skipped_results.append({
                        "keyword": keyword,
                        "query_count": query_count,
                        "total": 0
                    })
                else:
                    keywords_to_fetch.append(keyword)
                    query_counts_to_fetch.append(query_count)

    start = time.time()
    results = []

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —è–≤–ª—è—é—Ç—Å—è –∞—Ä—Ç–∏–∫—É–ª–æ–º
    for i in range(0, len(keywords_to_fetch), chunk_size):
        current_query_counts = query_counts_to_fetch[i:i + chunk_size]
        chunk = keywords_to_fetch[i:i + chunk_size]
        logger.info(f'–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {i // chunk_size + 1} ({len(chunk)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)')
        chunk_results = asyncio.run(scrape_all(chunk, concurrency, current_query_counts))
        results.extend(chunk_results)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ (–∞—Ä—Ç–∏–∫—É–ª—ã)
    results.extend(skipped_results)

    logger.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {time.time() - start:.2f} —Å–µ–∫. ‚Äî –≤—Å–µ–≥–æ: {len(results)} –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤")
    save_results(results, filename_out, fileformats)



def main():
    filename_input = input('–ò–º—è —Ñ–∞–π–ª–∞ (–±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è):')+'.csv'
    filename_out = f"out_{filename_input}"
    fileformats = ['csv']
    parse(filename_input, filename_out, fileformats, chunk_size=10**9, concurrency=120) # –±—ã–ª–æ 50
if __name__ == "__main__":
    #58/—Å–µ–∫, 75/—Å–µ–∫ (limit), 115/—Å–µ–∫ (10**9, conc = 200), 115(conn = 300, limit –≤—ã—à–µ), 129(conn = 100, limit –º–µ–Ω—å—à–µ), 140(conn = 100, limit = 200), 180(conn = 120, limit = 150)
    main()
